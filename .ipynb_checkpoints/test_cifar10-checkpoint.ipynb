{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:25: UserWarning: Detected an old version of PyTorch. Suggest using torch>=1.2.0 for the best experience.\n",
      "  warnings.warn(msg, warn_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "# Torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Torchvison\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import CIFAR100, CIFAR10,Caltech101\n",
    "\n",
    "# Utils\n",
    "import visdom\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom\n",
    "import models.resnet as resnet\n",
    "import models.lossnet as lossnet\n",
    "from config import *\n",
    "from data.sampler import SubsetSequentialSampler\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import arguments\n",
    "from utils import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from scipy import sparse\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, load_data\n",
    "\n",
    "from gcn import GCN\n",
    "from vat import VATLoss\n",
    "\n",
    "#1.seed 2.device 3.recifar 4.backends 5.mask\n",
    "# Seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed) #cpu\n",
    "torch.cuda.manual_seed_all(seed)  #并行gpu\n",
    "torch.backends.cudnn.deterministic = True  #cpu/gpu结果一致\n",
    "torch.backends.cudnn.benchmark = True   #训练集变化不大\n",
    "alpha = 0.3\n",
    "beta = 0.033\n",
    "accuracies = []\n",
    "\n",
    "##\n",
    "# Data\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(size=32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "])\n",
    "cifar10_test  = CIFAR10('./cifar10', train=False, download=True, transform=test_transform)\n",
    "cifar10_train = CIFAR10('./cifar10', train=True, download=True, transform=train_transform)\n",
    "\n",
    "class CIFAR10_re(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.cifar10 = CIFAR10(root=path,\n",
    "                                        download=True,\n",
    "                                        train=True,\n",
    "                                        transform=train_transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, np.float64):\n",
    "            index = index.astype(np.int64)\n",
    "\n",
    "        data, target = self.cifar10[index]\n",
    "\n",
    "        return data, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar10)\n",
    "\n",
    "cifar10_unlabeled   = CIFAR10_re('./cifar10')\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "##\n",
    "# Loss Prediction Loss\n",
    "def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n",
    "    assert len(input) % 2 == 0, 'the batch size is not even.'\n",
    "    assert input.shape == input.flip(0).shape\n",
    "    \n",
    "    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n",
    "        loss = loss / input.size(0) # Note that the size of input is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * input, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "##\n",
    "# Train Utils\n",
    "iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, vis=None, plot_data=None):\n",
    "    models['backbone'].train()\n",
    "    # models['module'].train()\n",
    "    global iters\n",
    "\n",
    "    for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n",
    "        inputs = data[0].cuda()\n",
    "        labels = data[1].cuda()\n",
    "        # print(inputs.shape, labels.shape)\n",
    "        iters += 1\n",
    "#         print(inputs.shape)\n",
    "        optimizers['backbone'].zero_grad()\n",
    "        # optimizers['module'].zero_grad()\n",
    "        models['backbone'].cuda()\n",
    "        scores, features, for_gcn = models['backbone'](inputs)\n",
    "        target_loss = criterion(scores, labels)\n",
    "\n",
    "        # if epoch > epoch_loss:\n",
    "        #     # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model.\n",
    "        #     features[0] = features[0].detach()\n",
    "        #     features[1] = features[1].detach()\n",
    "        #     features[2] = features[2].detach()\n",
    "        #     features[3] = features[3].detach()\n",
    "        # pred_loss = models['module'](features)\n",
    "        # pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "        # m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
    "        # m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=MARGIN)\n",
    "        # loss            = m_backbone_loss + WEIGHT * m_module_loss\n",
    "\n",
    "        target_loss.backward()\n",
    "        optimizers['backbone'].step()\n",
    "        # optimizers['module'].step()\n",
    "\n",
    "        # Visualize\n",
    "        # if (iters % 100 == 0) and (vis != None) and (plot_data != None):\n",
    "        #     plot_data['X'].append(iters)\n",
    "        #     plot_data['Y'].append([\n",
    "        #         m_backbone_loss.item(),\n",
    "        #         m_module_loss.item(),\n",
    "        #         loss.item()\n",
    "        #     ])\n",
    "        #     vis.line(\n",
    "        #         X=np.stack([np.array(plot_data['X'])] * len(plot_data['legend']), 1),\n",
    "        #         Y=np.array(plot_data['Y']),\n",
    "        #         opts={\n",
    "        #             'title': 'Loss over Time',\n",
    "        #             'legend': plot_data['legend'],\n",
    "        #             'xlabel': 'Iterations',\n",
    "        #             'ylabel': 'Loss',\n",
    "        #             'width': 1200,\n",
    "        #             'height': 390,\n",
    "        #         },\n",
    "        #         win=1\n",
    "        #     )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, vis, plot_data, cycle, accuracies):\n",
    "    print('>> Train a Model.')\n",
    "    best_acc = 0.\n",
    "    checkpoint_dir = os.path.join('./cifar10', 'train', 'weights')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        schedulers['backbone'].step()\n",
    "        # schedulers['module'].step()\n",
    "\n",
    "        train_epoch(models, criterion, optimizers, dataloaders, 1, epoch_loss, vis, plot_data)\n",
    "    #     print(accuracies, epoch)\n",
    "\n",
    "    # Save a checkpoint\n",
    "    # acc = test(models, dataloaders, 'val')\n",
    "    # if best_acc < acc:\n",
    "    #     best_acc = acc\n",
    "    #     best_model = copy.deepcopy(models['backbone'])\n",
    "    # # test_acc = test(models, dataloaders, 'test')\n",
    "    # print(acc, 'val_acc', best_acc, 'best_acc', 'accs', accuracies, 'epoch', epoch, 'cycle' ,cycle)\n",
    "\n",
    "\n",
    "\n",
    "    # if False and epoch % 5 == 4:\n",
    "    #     acc = test(models, dataloaders, 'test')\n",
    "    #     if best_acc < acc:\n",
    "    #         best_acc = acc\n",
    "    #         torch.save({\n",
    "    #             'epoch': epoch + 1,\n",
    "    #             'state_dict_backbone': models['backbone'].state_dict(),\n",
    "    #             'state_dict_module': models['module'].state_dict()\n",
    "    #         },\n",
    "    #         '%s/active_resnet18_cifar10.pth' % (checkpoint_dir))\n",
    "    #     print('Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc))\n",
    "\n",
    "    torch.save(models['backbone'].state_dict(), \"model_\" + str(cycle) + \".pt\")\n",
    "    print('>> Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, dataloaders, mode='val'):\n",
    "    assert mode == 'val' or mode == 'test'\n",
    "    models['backbone'].eval()\n",
    "    # models['module'].eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in dataloaders[mode]:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            scores, _, _ = models['backbone'](inputs)\n",
    "            _, preds = torch.max(scores.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "#\n",
    "#\n",
    "def get_uncertainty(models, unlabeled_loader):\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "    uncertainty = torch.tensor([]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in unlabeled_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            # labels = labels.cuda()\n",
    "\n",
    "            scores, features = models['backbone'](inputs)\n",
    "            pred_loss = models['module'](features) # pred_loss = criterion(scores, labels) # ground truth loss\n",
    "            pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "            uncertainty = torch.cat((uncertainty, pred_loss), 0)\n",
    "    \n",
    "    return uncertainty.cpu()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_MEAN = [0.5429, 0.5263, 0.4994]\n",
    "RGB_STD = [0.2422, 0.2392, 0.2406]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                            transforms.RandomResizedCrop(256, (.8, 1)),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.RandomRotation(degrees=15),\n",
    "                            transforms.ColorJitter(),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(RGB_MEAN, RGB_STD),\n",
    "                            ])\n",
    "\n",
    "# # train_transform = transforms.Compose(\n",
    "# #     [     transforms.Resize((224, 224)),\n",
    "# #      transforms.ToTensor(),\n",
    "# #      transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
    "# #      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "# #                           std=[0.229, 0.224, 0.225])])\n",
    "# cifar10_train = Caltech101('./caltech', download=False, transform=train_transform)\n",
    "# # cifar10_train = Caltech101('./caltech', download=False, transform=None)\n",
    "# print(cifar10_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train a Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:14<04:28, 14.13s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:14<02:48,  9.94s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:14<01:52,  7.03s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:14<01:14,  4.99s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:15<00:49,  3.57s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:15<00:33,  2.57s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:15<00:22,  1.87s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:23<00:39,  3.62s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [00:23<00:26,  2.63s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:23<00:17,  1.91s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [00:24<00:11,  1.41s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:24<00:07,  1.06s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [00:24<00:04,  1.23it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:24<00:03,  1.56it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:25<00:02,  1.92it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:28<00:04,  1.37s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:28<00:02,  1.03s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:28<00:00,  1.26it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:30<00:00,  1.09it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:12<03:49, 12.07s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:12<02:33,  8.51s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:12<01:42,  6.03s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:12<01:08,  4.29s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:13<00:46,  3.08s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:13<00:31,  2.23s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:13<00:21,  1.63s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:13<00:14,  1.21s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:23<00:40,  3.70s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [00:23<00:26,  2.64s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:23<00:17,  1.92s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [00:23<00:11,  1.41s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:24<00:07,  1.06s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [00:24<00:04,  1.23it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:24<00:03,  1.55it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:24<00:02,  1.91it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:28<00:04,  1.35s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:28<00:02,  1.02s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:28<00:00,  1.27it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.61it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:12<03:48, 12.03s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:12<02:32,  8.48s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:12<01:42,  6.01s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:12<01:08,  4.28s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:12<00:46,  3.07s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:13<00:31,  2.22s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:13<00:21,  1.63s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:13<00:14,  1.21s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:23<00:40,  3.71s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [00:23<00:26,  2.65s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:23<00:17,  1.93s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [00:23<00:11,  1.42s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:24<00:07,  1.07s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [00:24<00:04,  1.22it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:24<00:03,  1.55it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:24<00:02,  1.91it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:28<00:04,  1.35s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:28<00:02,  1.01s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:28<00:00,  1.28it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.63it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:12<03:52, 12.25s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:12<02:35,  8.62s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:12<01:43,  6.11s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:12<01:09,  4.35s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:13<00:46,  3.12s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:13<00:31,  2.25s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:13<00:21,  1.65s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:13<00:14,  1.23s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:23<00:41,  3.82s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [00:23<00:27,  2.73s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:24<00:17,  1.98s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [00:24<00:11,  1.46s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:24<00:07,  1.09s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [00:24<00:05,  1.19it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:25<00:03,  1.52it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:25<00:02,  1.87it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:28<00:04,  1.41s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:29<00:02,  1.05s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:29<00:00,  1.24it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:29<00:00,  1.58it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:12<03:51, 12.20s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:12<02:34,  8.59s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:12<01:43,  6.09s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:12<01:09,  4.33s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:13<00:46,  3.10s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:13<00:31,  2.25s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:13<00:21,  1.64s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:13<00:14,  1.22s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:23<00:41,  3.77s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [00:23<00:26,  2.69s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:23<00:17,  1.96s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [00:24<00:11,  1.44s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:24<00:07,  1.08s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [00:24<00:04,  1.21it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:24<00:03,  1.53it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:25<00:02,  1.89it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:28<00:04,  1.47s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:29<00:02,  1.09s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:29<00:00,  1.19it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:29<00:00,  1.53it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:12<03:51, 12.17s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:12<02:34,  8.56s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:12<01:43,  6.07s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:12<01:09,  4.32s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:13<00:46,  3.10s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:13<00:31,  2.24s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:13<00:21,  1.64s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:13<00:14,  1.22s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:23<00:40,  3.70s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [00:23<00:26,  2.69s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:23<00:17,  1.95s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [00:24<00:11,  1.44s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:24<00:07,  1.08s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [00:24<00:04,  1.21it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:24<00:03,  1.53it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:25<00:02,  1.89it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:28<00:04,  1.36s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:28<00:02,  1.02s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:28<00:00,  1.27it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:29<00:00,  1.61it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:12<03:50, 12.12s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:12<02:33,  8.53s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:12<01:42,  6.05s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:12<01:08,  4.30s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:13<00:46,  3.08s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:13<00:31,  2.23s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:13<00:21,  1.63s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:13<00:14,  1.22s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:23<00:41,  3.81s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [00:23<00:27,  2.78s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:24<00:18,  2.02s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [00:24<00:11,  1.49s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:24<00:07,  1.11s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [00:24<00:05,  1.18it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:25<00:03,  1.50it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:25<00:02,  1.86it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:28<00:04,  1.35s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:28<00:02,  1.02s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:29<00:00,  1.27it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:29<00:00,  1.61it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Main\n",
    "# if __name__ == '__main__':\n",
    "# args = arguments.get_args()\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "# main(args)\n",
    "# vis = visdom.Visdom(server='http://localhost', port=9000)\n",
    "vis = None\n",
    "plot_data = {'X': [], 'Y': [], 'legend': ['Backbone Loss', 'Module Loss', 'Total Loss']}\n",
    "\n",
    "for trial in range(TRIALS):\n",
    "    # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "\n",
    "    all_indices = set(np.arange(NUM_TRAIN))\n",
    "    val_indices = random.sample(all_indices, 0)\n",
    "    all_indices = np.setdiff1d(list(all_indices), val_indices)\n",
    "\n",
    "    initial_indices = random.sample(list(all_indices), 5000)\n",
    "    f = open(\"./init_indice.pkl\", 'wb')\n",
    "    pickle.dump(initial_indices, f)\n",
    "    # indices = all_indices\n",
    "    # random.shuffle(indices)\n",
    "    # labeled_set = indices[:ADDENDUM]\n",
    "\n",
    "    # unlabeled_set = indices[ADDENDUM:]\n",
    "\n",
    "    current_indices = list(initial_indices)\n",
    "\n",
    "    train_loader = DataLoader(cifar10_train, batch_size=BATCH, \n",
    "                              sampler=SubsetRandomSampler(initial_indices), \n",
    "                              pin_memory=True,drop_last=False, num_workers=8)\n",
    "    test_loader  = DataLoader(cifar10_train, batch_size=BATCH, num_workers=8)\n",
    "    val_loader = DataLoader(cifar10_train, batch_size=BATCH, \n",
    "                              sampler=SubsetRandomSampler(val_indices), \n",
    "                              pin_memory=True, \n",
    "                           num_workers=8)\n",
    "    dataloaders  = {'train': train_loader, 'test': test_loader, 'val': val_loader}\n",
    "\n",
    "    # Model\n",
    "    # resnet18    = resnet.ResNet18(num_classes=10).cuda()\n",
    "    # # loss_module = lossnet.LossNet().cuda()\n",
    "    # models      = {'backbone': resnet18}\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Active learning cycles\n",
    "\n",
    "#         for cycle in range(CYCLES):\n",
    "    # Loss, criterion and scheduler (re)initialization\n",
    "\n",
    "    # Randomly sample 10000 unlabeled data points\n",
    "    # random.shuffle(unlabeled_set)\n",
    "    unlabeled_indices = np.setdiff1d(list(all_indices), current_indices)\n",
    "    # Create unlabeled dataloader for the unlabeled subset\n",
    "    unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH, \n",
    "                                  sampler=SubsetSequentialSampler(unlabeled_indices), # more convenient if we maintain the order of subset\n",
    "                                  pin_memory=True)\n",
    "\n",
    "    resnet18    = resnet.ResNet18(num_classes=102).cuda()\n",
    "    # loss_module = lossnet.LossNet().cuda()\n",
    "    models      = {'backbone': resnet18}\n",
    "    criterion      = nn.CrossEntropyLoss()\n",
    "    optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, \n",
    "                            momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "    # optim_module   = optim.SGD(models['module'].parameters(), lr=LR, \n",
    "    #                         momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "    sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n",
    "    # sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=MILESTONES)\n",
    "\n",
    "    optimizers = {'backbone': optim_backbone}\n",
    "    schedulers = {'backbone': sched_backbone}\n",
    "\n",
    "    # Training and test\n",
    "    train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL, vis, plot_data, 0, accuracies)\n",
    "    acc = test(models, dataloaders, mode='test')\n",
    "    accuracies.append(acc)\n",
    "    print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial+1, TRIALS, cycle+1, CYCLES, len(current_indices), acc))\n",
    "\n",
    "\n",
    "    #semi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:00<00:20,  4.79it/s]\u001b[A\n",
      "  2%|▏         | 2/100 [00:00<00:20,  4.82it/s]\u001b[A\n",
      "  3%|▎         | 3/100 [00:00<00:19,  4.90it/s]\u001b[A\n",
      "  4%|▍         | 4/100 [00:00<00:19,  4.96it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:01<00:18,  5.01it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:01<00:18,  5.01it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:01<00:18,  5.06it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:01<00:18,  5.10it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:01<00:17,  5.09it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:02<00:19,  4.66it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:02<00:18,  4.74it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:02<00:18,  4.88it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:02<00:17,  4.98it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:02<00:17,  5.06it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:03<00:16,  5.05it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:03<00:16,  5.08it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:03<00:16,  5.07it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:03<00:15,  5.13it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:03<00:15,  5.17it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:03<00:15,  5.19it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:04<00:15,  5.21it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:04<00:14,  5.23it/s]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:04<00:14,  5.24it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:04<00:14,  5.23it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:04<00:14,  5.24it/s]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:05<00:14,  5.23it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:05<00:13,  5.24it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:05<00:13,  5.19it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:05<00:13,  5.13it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:05<00:13,  5.15it/s]\u001b[A\n",
      " 31%|███       | 31/100 [00:06<00:13,  5.14it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:06<00:13,  5.16it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:06<00:13,  5.13it/s]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:06<00:12,  5.13it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:06<00:12,  5.15it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:07<00:12,  5.18it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:07<00:12,  5.19it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:07<00:11,  5.22it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:07<00:11,  5.23it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:07<00:11,  5.25it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:08<00:11,  5.25it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:08<00:11,  5.22it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:08<00:10,  5.21it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:08<00:10,  5.20it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:08<00:10,  5.22it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:08<00:10,  5.22it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:09<00:10,  5.23it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:09<00:09,  5.23it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:09<00:09,  5.18it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:09<00:09,  5.15it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:09<00:09,  5.14it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:10<00:09,  5.18it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:10<00:09,  5.19it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:10<00:08,  5.18it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:10<00:08,  5.20it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:10<00:08,  5.22it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:11<00:08,  5.21it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:11<00:08,  5.22it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:11<00:07,  5.21it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:11<00:07,  5.21it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:11<00:07,  5.22it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:12<00:07,  5.20it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:12<00:07,  5.14it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:12<00:07,  5.10it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:12<00:06,  5.13it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:12<00:06,  5.10it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:13<00:06,  5.10it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:13<00:06,  5.08it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:13<00:06,  5.08it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:13<00:05,  5.08it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:13<00:05,  5.10it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:14<00:05,  5.15it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:14<00:05,  5.19it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:14<00:05,  5.20it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:14<00:04,  5.19it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:14<00:04,  5.21it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:14<00:04,  5.22it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:15<00:04,  5.23it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:15<00:03,  5.26it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:15<00:03,  5.25it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:15<00:03,  5.26it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:15<00:03,  5.24it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:16<00:03,  5.24it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:16<00:03,  5.26it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:16<00:02,  5.24it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:16<00:02,  5.23it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:16<00:02,  5.24it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [00:17<00:02,  5.25it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:17<00:02,  5.21it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:17<00:01,  5.23it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:17<00:01,  5.21it/s]\u001b[A\n",
      " 92%|█████████▏| 92/100 [00:17<00:01,  5.15it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:18<00:01,  5.15it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:18<00:01,  5.17it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:18<00:00,  5.18it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:18<00:00,  5.21it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [00:18<00:00,  5.21it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:18<00:00,  5.18it/s]\u001b[A\n",
      " 99%|█████████▉| 99/100 [00:19<00:00,  5.21it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:19<00:00,  5.16it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    a = torch.randn([9000, 1000]).cuda()\n",
    "    b = torch.randn([1000, 9000]).cuda()\n",
    "    c = torch.matmul(a, b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
